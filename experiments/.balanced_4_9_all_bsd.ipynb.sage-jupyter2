{"backend_state":"running","connection_file":"/projects/bda83a78-8667-461e-b29a-519419abf9c8/.local/share/jupyter/runtime/kernel-1368abb7-b769-43f6-8c1c-02c9cd4ab61c.json","kernel":"python3-sage","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1734120240896,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":10,"id":"ff22b8","input":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate mean squared error\nmse = mean_squared_error(y_test, y_pred)\n\n# Display the mean squared error\nprint(\"Mean Squared Error:\", mse)\n\n# Display the linear regression equation coefficients\nprint(\"Intercept:\", model.intercept_)\nprint(\"Coefficients:\", model.coef_)\n\n# Display the equation\nequation = \"y = {:.4f} + \".format(model.intercept_)\nequation += \" + \".join([\"{:.4f} * {}\".format(coef, feature) for coef, feature in zip(model.coef_, X.columns)])\nprint(\"Linear Regression Equation:\", equation)","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Mean Squared Error: 9.191485945949486e-09\nIntercept: 5.026547875264242e-06\nCoefficients: [ 0.99999986  2.00000005 -0.99999814 -0.99999949 -1.00000108]\nLinear Regression Equation: y = 0.0000 + 1.0000 * special_value + 2.0000 * torsion + -1.0000 * real_period + -1.0000 * regulator + -1.0000 * tamagawa_product\n"}},"pos":6,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":15,"id":"1c4fe0","input":"from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Splitting features and target\nX = df_balanced_bsd[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_balanced_bsd['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Histogram-based Gradient Boosting classifier\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(y_test, y_pred))","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Accuracy: 0.9828970850684117\n              precision    recall  f1-score   support\n\n           4       0.98      0.99      0.98     10173\n           9       0.99      0.98      0.98      9999\n\n    accuracy                           0.98     20172\n   macro avg       0.98      0.98      0.98     20172\nweighted avg       0.98      0.98      0.98     20172\n\n"}},"pos":7.5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":16,"id":"74bac9","input":"# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Convert the log-transformed 'sha' column back to categorical labels\ndf_log_transformed['sha'] = df_balanced_bsd['sha']\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Histogram-based Gradient Boosting classifier\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(y_test, y_pred))","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Accuracy: 0.9828970850684117\n              precision    recall  f1-score   support\n\n           4       0.98      0.99      0.98     10173\n           9       0.99      0.98      0.98      9999\n\n    accuracy                           0.98     20172\n   macro avg       0.98      0.98      0.98     20172\nweighted avg       0.98      0.98      0.98     20172\n\n"}},"pos":7.6875,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":3,"id":"75e194","input":"from lib import utils, models, executor\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","kernel":"python3-sage","pos":0,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":4,"id":"4c9151","input":"# load your data here. The following ensure this will work on Windows as well as Unix\n\npath = Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_reg.parquet\"\ndf = utils.load_data(path)","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Loaded the dataset with 120 features and 3064705 curves..\n"}},"pos":1,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":5,"id":"1d5832","input":"# The first experiment will be to take a balanced 4/9 dataset with all the BSD features, and only the BSD features (i.e. no ap vals). This is a sanity check\n\nlen_9 = df[df['sha'] == 9].shape[0]\ndf_balanced = df[df['sha'] == 4].sample(len_9) \ndf_balanced = pd.concat([df_balanced, df[df['sha'] == 9]])\ndf_balanced.sha.value_counts()","kernel":"python3-sage","output":{"0":{"data":{"text/plain":"sha\n4    50428\n9    50428\nName: count, dtype: int64"},"exec_count":5}},"pos":2,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":6,"id":"76c754","input":"bsd_features = ['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product', 'sha']\n\ndf_balanced_bsd = df_balanced[bsd_features].copy()","kernel":"python3-sage","pos":3,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":7,"id":"271b02","input":"df_balanced_bsd.head(5)","kernel":"python3-sage","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>special_value</th>\n      <th>torsion</th>\n      <th>real_period</th>\n      <th>regulator</th>\n      <th>tamagawa_product</th>\n      <th>sha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1648936</th>\n      <td>4.77592</td>\n      <td>2</td>\n      <td>0.14925</td>\n      <td>1.0</td>\n      <td>32</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>262328</th>\n      <td>6.08918</td>\n      <td>1</td>\n      <td>0.16914</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1667600</th>\n      <td>2.41156</td>\n      <td>2</td>\n      <td>0.30145</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2134251</th>\n      <td>3.07913</td>\n      <td>2</td>\n      <td>0.38489</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1054165</th>\n      <td>1.35681</td>\n      <td>2</td>\n      <td>0.16960</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         special_value  torsion  real_period  regulator  tamagawa_product  sha\n1648936        4.77592        2      0.14925        1.0                32    4\n262328         6.08918        1      0.16914        1.0                 9    4\n1667600        2.41156        2      0.30145        1.0                 8    4\n2134251        3.07913        2      0.38489        1.0                 8    4\n1054165        1.35681        2      0.16960        1.0                 8    4"},"exec_count":7}},"pos":4,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":8,"id":"30a990","input":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Convert the log-transformed 'sha' column back to categorical labels\ndf_log_transformed['sha'] = df_balanced_bsd['sha']\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Accuracy: 1.0\n"}},"pos":5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":9,"id":"49cec2","input":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Splitting features and target\nX = df_balanced_bsd[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_balanced_bsd['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","kernel":"python3-sage","output":{"0":{"name":"stdout","text":"Accuracy: 0.6395994447749356\n"}},"pos":4.5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3fcb02","input":"### The following cell runs logistic regression on the data as-is, and yields about 64% accuracy.","pos":4.25,"type":"cell"}
{"cell_type":"markdown","id":"4cd563","input":"### Do the same as above, but this time take the logarithm of the data","pos":4.75,"type":"cell"}
{"cell_type":"markdown","id":"67b46a","input":"## Create the balanced dataset","pos":0.5,"type":"cell"}
{"cell_type":"markdown","id":"9882e7","input":"### As mentioned in the paper, taking the log of the data makes no difference","pos":7.59375,"type":"cell"}
{"cell_type":"markdown","id":"9f1e08","input":"### Run a histogram-based gradient boosting machine on the data","pos":6.75,"type":"cell"}
{"cell_type":"markdown","id":"dcec17","input":"### Run a linear regression model and give the relationship it thinks exists between the BSD features. It yields the BSD formula. As written in the paper, this is not evidence for BSD.","pos":5.5,"type":"cell"}
{"id":0,"time":1734120175163,"type":"user"}
{"last_load":1729881022369,"type":"file"}