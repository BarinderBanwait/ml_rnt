{"backend_state":"running","connection_file":"/projects/bda83a78-8667-461e-b29a-519419abf9c8/.local/share/jupyter/runtime/kernel-6adc61ee-6f84-4d4b-b551-6763b295c269.json","kernel":"python3-sage","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1734320304622,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1734300179252,"exec_count":3,"id":"75e194","input":"from lib import utils\nfrom lib import models\nfrom lib import executor\nfrom lib.utils import nearest_integer_acc\n\nimport torch\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score","kernel":"python3-sage","pos":0,"start":1734300172966,"state":"done","type":"cell"}
{"cell_type":"code","end":1734300180258,"exec_count":4,"id":"4c9151","input":"# load your data here. The following ensure this will work on Windows as well as Unix\n# the data file has its Kodaira symbols already preprocessed \npath = '../data_files/sha/ecq_sha_B_100_conds_1_500000_reg.parquet'\n\nbsd_columns = ['rank', 'conductor', 'special_value', 'torsion', 'regulator', 'real_period', 'tamagawa_product','sha']\n# Read the specified columns using PyArrow\ntable = pq.read_table(path, columns=bsd_columns)\n\n# Convert the PyArrow Table to a Pandas DataFrame\ndf = table.to_pandas()","kernel":"python3-sage","pos":1,"start":1734300179952,"state":"done","type":"cell"}
{"cell_type":"code","end":1734300183142,"exec_count":5,"id":"83bbdc","input":"df['sha'].value_counts()","kernel":"python3-sage","output":{"0":{"data":{"text/plain":"sha\n1       2821178\n4        158758\n9         50428\n16        18170\n25         8501\n36         2402\n49         2153\n64         1301\n81          595\n100         315\n121         300\n169         160\n144         145\n196          58\n256          45\n225          43\n289          34\n361          29\n400          15\n529          13\n625          12\n324           7\n784           7\n441           5\n1024          4\n576           4\n729           4\n676           3\n961           3\n841           3\n484           2\n1681          1\n5625          1\n2209          1\n2500          1\n1089          1\n1849          1\n1369          1\n1156          1\nName: count, dtype: int64"},"exec_count":5}},"pos":2,"start":1734300183131,"state":"done","type":"cell"}
{"cell_type":"code","end":1734300187721,"exec_count":6,"id":"653060","input":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt","kernel":"python3-sage","pos":4,"start":1734300187718,"state":"done","type":"cell"}
{"cell_type":"code","end":1734300262651,"exec_count":7,"id":"07aa99","input":"# Extract features and labels\nfeatures = ['tamagawa_product', 'rank', 'conductor', 'special_value', 'torsion', 'regulator', 'real_period']\nX = df[features]\ny = df['sha']\n\n# Identify the top 10 classes by size\ntop_classes = y.value_counts().head(10).index\n\n# Filter the dataset to include only the top 10 classes\nfiltered_df = df[df['sha'].isin(top_classes)]\nX = filtered_df[features].copy()\ny = filtered_df['sha'].copy()\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Perform PCA\npca = PCA(n_components=2)  # Focus on the first two principal components\nX_pca = pca.fit_transform(X_scaled)\n\n# Plot the PCA results\nplt.figure(figsize=(8, 6))\nfor label in y.unique():\n    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=f'sha = {label}')\nplt.title('PCA of Features Colored by sha')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n\n# Investigate loadings\nloadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features)\nprint(\"PCA Loadings:\")\nprint(loadings)\n","kernel":"python3-sage","output":{"0":{"data":{"image/png":"18d2dc465d7bb525afcdd01291453f31bef20531","text/plain":"<Figure size 800x600 with 1 Axes>"},"metadata":{"image/png":{"height":546,"width":707}}},"1":{"name":"stdout","text":"PCA Loadings:\n                       PC1       PC2\ntamagawa_product  0.054891 -0.488592\nrank              0.653398  0.041912\nconductor         0.197856 -0.303398\nspecial_value     0.661104 -0.128973\ntorsion          -0.178066 -0.483496\nregulator         0.174262 -0.265682\nreal_period       0.178266  0.588626\n"}},"pos":5,"start":1734300233802,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"06fef8","input":"df_balanced_trunc.head(5)","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"17fcbf","input":"","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1c4918","input":"# choose model parameters\nhidden_units = [128,64,32]\n\n# default model parameters\ninput_dim, output_dim = utils.get_input_output_dim(df_balanced_bsd_no_reg, 'sha', if_regression=True)\n\n# check if we have cuda available\ndevice = utils.get_device()\n\n# create model\nmodel = models.VanillaNN(input_dim, hidden_units, output_dim, if_dropout=False, dropout_rate=0.5, if_batchnorm=True).to(device)\n\n# print model summary\nutils.model_summary(model)","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"30a990","input":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Convert the log-transformed 'sha' column back to categorical labels\ndf_log_transformed['sha'] = df_balanced_bsd['sha']\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"394211","input":"# Log transform only the specified columns\ncolumns_to_log_transform = ['special_value', 'torsion', 'real_period', 'tamagawa_product', 'sha']\nB = 10\ndf_balanced_trunc = df_balanced[[str(p) for p in prime_range(B)] + bsd_features].copy()\n\ndf_log_transformed = df_balanced_trunc.copy()\ndf_log_transformed[columns_to_log_transform] = df_log_transformed[columns_to_log_transform].apply(np.log)\n\n# Convert the log-transformed 'sha' column back to categorical labels\ndf_log_transformed['sha'] = df_balanced_trunc['sha']\n\n# Splitting features and target\nX = df_log_transformed[[str(p) for p in prime_range(B)] + ['special_value', 'torsion', 'real_period', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"3a5c0d","input":"bsd_features = ['special_value', 'torsion', 'real_period', 'tamagawa_product', 'sha']\nfrom sage.all import prime_range\nB = 12\n\ndf_balanced_trunc = df_balanced[[str(p) for p in prime_range(B)] + bsd_features]","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"407ab7","input":"","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"543e07","input":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Convert the log-transformed 'sha' column back to categorical labels\ndf_log_transformed['sha'] = df_balanced_bsd['sha']\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"616423","input":"# dropping columns that are not needed\ndf.drop(['conductor','adelic_level'], axis=1, inplace=True)\n\n# process kodaira symbol if it is not already done\n# path = Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000.parquet\"\n# df = utils.load_data(path)\n# df = utils.process_kodaira_symbol(df)\n# df.drop('lmfdb_label', axis=1, inplace=True)\n# df.to_parquet(Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_kodaira_processed.parquet\")","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"66f828","input":"","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"677626","input":"","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6b7b60","input":"","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7b1b1d","input":"from torch.utils.tensorboard import SummaryWriter\n\n# Load the dataframe with df_balanced_bsd_no_reg\ndf_balanced_trunc = df_balanced_trunc.copy()\n\n# Log transform the feature columns but leave the label as-is\ncolumns_to_log_transform = ['special_value', 'torsion', 'real_period', 'tamagawa_product']\ndf_log_transformed = df_balanced_trunc.copy()\ndf_log_transformed[columns_to_log_transform] = df_log_transformed[columns_to_log_transform].apply(np.log)\n\n# Splitting features and target\nX = df_log_transformed[columns_to_log_transform]\ny = df_log_transformed['sha']\n\n# Map labels to range starting from 0\ny = y.map({4: 0, 9: 1})\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n\n# Verify label values are in the correct range\nprint(\"Unique labels in training set:\", y_train_tensor.unique())\nprint(\"Unique labels in test set:\", y_test_tensor.unique())\n\n# Create DataLoader for training\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Define the neural network model\nclass VanillaNN(nn.Module):\n    def __init__(self, input_dim, hidden_units, output_dim, if_dropout=False, dropout_rate=0.5, if_batchnorm=False):\n        super(VanillaNN, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_units = hidden_units\n        self.output_dim = output_dim\n        self.if_dropout = if_dropout\n        self.dropout_rate = dropout_rate\n        self.if_batchnorm = if_batchnorm\n        self.layers = nn.ModuleList()\n\n        # Input layer\n        self.layers.append(nn.Linear(self.input_dim, self.hidden_units[0]))\n\n        # Hidden layers\n        for i in range(1, len(self.hidden_units)):\n            if self.if_batchnorm:\n                self.layers.append(nn.BatchNorm1d(self.hidden_units[i-1]))\n            if self.if_dropout:\n                self.layers.append(nn.Dropout(self.dropout_rate))\n            self.layers.append(nn.Linear(self.hidden_units[i-1], self.hidden_units[i]))\n\n        # Output layer\n        if self.if_batchnorm:\n            self.layers.append(nn.BatchNorm1d(self.hidden_units[-1]))\n        if self.if_dropout:\n            self.layers.append(nn.Dropout(self.dropout_rate))\n        self.layers.append(nn.Linear(self.hidden_units[-1], self.output_dim))\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            if isinstance(layer, nn.Linear):\n                x = torch.relu(x)\n        x = self.layers[-1](x)\n        return x\n\n# Initialize model, loss function, and optimizer\ninput_dim = X_train.shape[1]\nhidden_units = [128, 64, 32]\noutput_dim = len(y.unique())\ndevice = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\nmodel = VanillaNN(input_dim, hidden_units, output_dim).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Set up TensorBoard writer\nwriter = SummaryWriter(log_dir='runs/training_logs')\n\n# Train the model\nmodel.to(device)\n\nfor epoch in range(1000):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n        # Forward pass\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct_train += (predicted == y_batch).sum().item()\n        total_train += y_batch.size(0)\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct_train / total_train\n    \n    # Evaluate on test set\n    model.eval()\n    with torch.no_grad():\n        X_test_tensor = X_test_tensor.to(device)\n        y_test_tensor = y_test_tensor.to(device)\n        outputs = model(X_test_tensor)\n        test_loss = criterion(outputs, y_test_tensor).item()\n        _, y_pred = torch.max(outputs, 1)\n        test_accuracy = accuracy_score(y_test_tensor.cpu(), y_pred.cpu())\n\n    # Log metrics to TensorBoard\n    writer.add_scalar('Loss/Train', train_loss, epoch + 1)\n    writer.add_scalar('Loss/Test', test_loss, epoch + 1)\n    writer.add_scalar('Accuracy/Train', train_accuracy, epoch + 1)\n    writer.add_scalar('Accuracy/Test', test_accuracy, epoch + 1)\n\n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch [{epoch+1}/100], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n\n# Close the TensorBoard writer\nwriter.close()\n\n# Display the final accuracy score\nprint(\"Final Test Accuracy:\", test_accuracy)","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8452aa","input":"df_balanced_bsd_no_reg = df_balanced_bsd[['special_value', 'torsion', 'real_period', 'tamagawa_product', 'sha']].copy()","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8b658e","input":"# split data\ntrain_dataloader, val_dataset, test_dataset = utils.prepare_data(df_balanced_bsd_no_reg, 'sha', device, if_regression=False)\n# train the model\nmodel, train_eval_hist, val_eval_hist, train_loss_hist, val_loss_hist = executor.train(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression=False, verbose=True)\n# plot train_eval_hist, val_eval_hist\nutils.plot_train_eval_hist(train_eval_hist, val_eval_hist)","pos":44,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9af643","input":"","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9e0558","input":"","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a3c1b3","input":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Set the desired GPU (e.g., GPU 2)\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Set CUDA_LAUNCH_BLOCKING for debugging\n# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Load the dataframe with df_balanced_bsd_no_reg\ndf_balanced_bsd_no_reg = df_balanced_bsd_no_reg.copy()\n\n# Log transform the feature columns but leave the label as-is\ncolumns_to_log_transform = ['special_value', 'torsion', 'real_period', 'tamagawa_product']\ndf_log_transformed = df_balanced_bsd_no_reg.copy()\ndf_log_transformed[columns_to_log_transform] = df_log_transformed[columns_to_log_transform].apply(np.log)\n\n# Splitting features and target\nX = df_log_transformed[columns_to_log_transform]\ny = df_log_transformed['sha']\n\n# Map labels to range starting from 0\ny = y.map({4: 0, 9: 1})\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n\n# Verify label values are in the correct range\nprint(\"Unique labels in training set:\", y_train_tensor.unique())\nprint(\"Unique labels in test set:\", y_test_tensor.unique())\n\n# Create DataLoader for training\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Define the neural network model\nclass VanillaNN(nn.Module):\n    def __init__(self, input_dim, hidden_units, output_dim, if_dropout=False, dropout_rate=0.5, if_batchnorm=False):\n        super(VanillaNN, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_units = hidden_units\n        self.output_dim = output_dim\n        self.if_dropout = if_dropout\n        self.dropout_rate = dropout_rate\n        self.if_batchnorm = if_batchnorm\n        self.layers = nn.ModuleList()\n\n        # Input layer\n        self.layers.append(nn.Linear(self.input_dim, self.hidden_units[0]))\n\n        # Hidden layers\n        for i in range(1, len(self.hidden_units)):\n            if self.if_batchnorm:\n                self.layers.append(nn.BatchNorm1d(self.hidden_units[i-1]))\n            if self.if_dropout:\n                self.layers.append(nn.Dropout(self.dropout_rate))\n            self.layers.append(nn.Linear(self.hidden_units[i-1], self.hidden_units[i]))\n\n        # Output layer\n        if self.if_batchnorm:\n            self.layers.append(nn.BatchNorm1d(self.hidden_units[-1]))\n        if self.if_dropout:\n            self.layers.append(nn.Dropout(self.dropout_rate))\n        self.layers.append(nn.Linear(self.hidden_units[-1], self.output_dim))\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            if isinstance(layer, nn.Linear):\n                x = torch.relu(x)\n        x = self.layers[-1](x)\n        return x\n\n# Initialize model, loss function, and optimizer\ninput_dim = X_train.shape[1]\nhidden_units = [128, 64, 32]\noutput_dim = len(y.unique())\ndevice = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\nmodel = VanillaNN(input_dim, hidden_units, output_dim).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nmodel.to(device)\n\nfor epoch in range(100):\n    model.train()\n    running_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n        # Forward pass\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/100], Loss: {running_loss/len(train_loader):.4f}\")\n\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    X_test_tensor = X_test_tensor.to(device)\n    y_test_tensor = y_test_tensor.to(device)\n    outputs = model(X_test_tensor)\n    _, y_pred = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test_tensor.cpu(), y_pred.cpu())\n\n# Display the accuracy score\nprint(\"Accuracy:\", accuracy)","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"acb23d","input":"acc = executor.test(model, test_dataset, evaluator, if_regression = True)\nmae = executor.test(model, test_dataset, mean_absolute_error, if_regression = True)\nmse = executor.test(model, test_dataset, mean_squared_error, if_regression = True)\nprint(f\"Test accuracy: {acc:0.4f}\")\nprint(f\"Test Mean Absolute Error: {mae:0.4f}. Test Mean Squared Error: {mse:0.4f}\")","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ba0d85","input":"# Display the linear regression equation coefficients\nprint(\"Intercept:\", model.intercept_)\nprint(\"Coefficients:\", model.coef_)\n\n# Display the equation\nequation = \"y = {:.4f} + \".format(model.intercept_)\nequation += \" + \".join([\"{:.4f} * {}\".format(coef, feature) for coef, feature in zip(model.coef_, X.columns)])\nprint(\"Linear Regression Equation:\", equation)","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c071ec","input":"# choose training parameters\nloss_func = nn.MSELoss()\nnum_epochs = 30\nlr = 0.001\noptimizer = optim.Adam(model.parameters(), lr=lr)\nevaluator = perfect_square_acc","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"de3eb6","input":"","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e62c10","input":"","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e9f478","input":"","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f55ab3","input":"","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f9025f","input":"utils.plot_train_loss_hist(train_loss_hist, val_loss_hist)","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ff22b8","input":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Log transform the dataframe\ndf_log_transformed = df_balanced_bsd.apply(np.log)\n\n# Splitting features and target\nX = df_log_transformed[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_log_transformed['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate mean squared error\nmse = mean_squared_error(y_test, y_pred)\n\n# Display the mean squared error\nprint(\"Mean Squared Error:\", mse)","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"d59b86","input":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\n# Define features and target\nX = df[['torsion', 'real_period']]\ny = df['rank']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train decision tree\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = tree.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Visualize the decision tree\nplt.figure(figsize=(10, 6))\nplot_tree(tree, feature_names=['torsion', 'real_period'], class_names=['Rank 1', 'Rank 2', 'Rank 3'], filled=True)\nplt.title('Decision Tree Visualization')\nplt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.5013859408980637\n"},"1":{"name":"stderr","output_type":"stream","text":"/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"},"2":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00    233861\n           1       0.50      1.00      0.67    307320\n           2       0.00      0.00      0.00     69826\n           3       0.00      0.00      0.00      1933\n           4       0.00      0.00      0.00         1\n\n    accuracy                           0.50    612941\n   macro avg       0.10      0.20      0.13    612941\nweighted avg       0.25      0.50      0.33    612941\n\n"},"3":{"data":{"image/png":"ad6fb4c548675dbfee820b8c50cb3663de9a9251","text/plain":"<Figure size 1000x600 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":503,"width":794}},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"b402d3","input":"df['rank'].value_counts()","output":{"0":{"data":{"text/plain":"rank\n1    1535669\n0    1170876\n2     348672\n3       9487\n4          1\nName: count, dtype: int64"},"exec_count":12,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"26f3f9","input":"from sklearn.ensemble import RandomForestClassifier\n\n# Train a Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Evaluate\ny_pred_rf = rf.predict(X_test)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))\n\n# Feature importance\nimportances = rf.feature_importances_\nprint(\"Feature Importances:\", dict(zip(['torsion', 'real_period'], importances)))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Random Forest Accuracy: 0.47513382201549575\n"},"1":{"name":"stderr","output_type":"stream","text":"/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"},"2":{"name":"stderr","output_type":"stream","text":"/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/sage/local/var/lib/sage/venv-python3.12.4/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"},"3":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.43      0.40      0.42    233861\n           1       0.52      0.61      0.57    307320\n           2       0.23      0.11      0.15     69826\n           3       0.04      0.02      0.03      1933\n           4       0.00      0.00      0.00         1\n\n    accuracy                           0.48    612941\n   macro avg       0.25      0.23      0.23    612941\nweighted avg       0.45      0.48      0.46    612941\n\n"},"4":{"name":"stdout","output_type":"stream","text":"Feature Importances: {'torsion': 0.010549971509196046, 'real_period': 0.989450028490804}\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"02696f","input":"df['sha_group'] = pd.cut(df['sha'], bins=[0, 10, 100, 10000], labels=['Low', 'Medium', 'High'])\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['sha_group'])\nplt.title(\"PCA of Features Grouped by sha\")\nplt.show()","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"4fede9","input":"df_balanced.to_csv('tamagawa_investigation.csv', index=False)","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"10320a","input":"correlations = df_balanced.corr()\nprint(correlations['special_value'])","output":{"0":{"name":"stdout","output_type":"stream","text":"rank                0.258719\nconductor           0.246658\nspecial_value       1.000000\ntorsion            -0.031803\nregulator           0.025147\nreal_period        -0.017403\ntamagawa_product    0.120961\nsha                 0.030097\nName: special_value, dtype: float64\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"61e0e2","input":"correlations","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>conductor</th>\n      <th>special_value</th>\n      <th>torsion</th>\n      <th>regulator</th>\n      <th>real_period</th>\n      <th>tamagawa_product</th>\n      <th>sha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rank</th>\n      <td>1.000000</td>\n      <td>0.004457</td>\n      <td>0.258719</td>\n      <td>-0.165730</td>\n      <td>-0.030894</td>\n      <td>0.265980</td>\n      <td>0.029366</td>\n      <td>-0.320757</td>\n    </tr>\n    <tr>\n      <th>conductor</th>\n      <td>0.004457</td>\n      <td>1.000000</td>\n      <td>0.246658</td>\n      <td>-0.011310</td>\n      <td>0.042001</td>\n      <td>-0.150868</td>\n      <td>0.009983</td>\n      <td>0.126790</td>\n    </tr>\n    <tr>\n      <th>special_value</th>\n      <td>0.258719</td>\n      <td>0.246658</td>\n      <td>1.000000</td>\n      <td>-0.031803</td>\n      <td>0.025147</td>\n      <td>-0.017403</td>\n      <td>0.120961</td>\n      <td>0.030097</td>\n    </tr>\n    <tr>\n      <th>torsion</th>\n      <td>-0.165730</td>\n      <td>-0.011310</td>\n      <td>-0.031803</td>\n      <td>1.000000</td>\n      <td>0.035885</td>\n      <td>-0.269792</td>\n      <td>0.118903</td>\n      <td>0.319142</td>\n    </tr>\n    <tr>\n      <th>regulator</th>\n      <td>-0.030894</td>\n      <td>0.042001</td>\n      <td>0.025147</td>\n      <td>0.035885</td>\n      <td>1.000000</td>\n      <td>-0.122537</td>\n      <td>-0.024504</td>\n      <td>0.039164</td>\n    </tr>\n    <tr>\n      <th>real_period</th>\n      <td>0.265980</td>\n      <td>-0.150868</td>\n      <td>-0.017403</td>\n      <td>-0.269792</td>\n      <td>-0.122537</td>\n      <td>1.000000</td>\n      <td>-0.090057</td>\n      <td>-0.345632</td>\n    </tr>\n    <tr>\n      <th>tamagawa_product</th>\n      <td>0.029366</td>\n      <td>0.009983</td>\n      <td>0.120961</td>\n      <td>0.118903</td>\n      <td>-0.024504</td>\n      <td>-0.090057</td>\n      <td>1.000000</td>\n      <td>-0.077437</td>\n    </tr>\n    <tr>\n      <th>sha</th>\n      <td>-0.320757</td>\n      <td>0.126790</td>\n      <td>0.030097</td>\n      <td>0.319142</td>\n      <td>0.039164</td>\n      <td>-0.345632</td>\n      <td>-0.077437</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                      rank  conductor  special_value   torsion  regulator  \\\nrank              1.000000   0.004457       0.258719 -0.165730  -0.030894   \nconductor         0.004457   1.000000       0.246658 -0.011310   0.042001   \nspecial_value     0.258719   0.246658       1.000000 -0.031803   0.025147   \ntorsion          -0.165730  -0.011310      -0.031803  1.000000   0.035885   \nregulator        -0.030894   0.042001       0.025147  0.035885   1.000000   \nreal_period       0.265980  -0.150868      -0.017403 -0.269792  -0.122537   \ntamagawa_product  0.029366   0.009983       0.120961  0.118903  -0.024504   \nsha              -0.320757   0.126790       0.030097  0.319142   0.039164   \n\n                  real_period  tamagawa_product       sha  \nrank                 0.265980          0.029366 -0.320757  \nconductor           -0.150868          0.009983  0.126790  \nspecial_value       -0.017403          0.120961  0.030097  \ntorsion             -0.269792          0.118903  0.319142  \nregulator           -0.122537         -0.024504  0.039164  \nreal_period          1.000000         -0.090057 -0.345632  \ntamagawa_product    -0.090057          1.000000 -0.077437  \nsha                 -0.345632         -0.077437  1.000000  "},"exec_count":23,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"eeac77","input":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(x='sha', y='special_value', data=df_balanced)\nplt.title('Distribution of special_value by sha')\nplt.show()\n","output":{"0":{"data":{"image/png":"c969c0e945f201b79cbd2950ce834ce47584a484","text/plain":"<Figure size 1200x700 with 1 Axes>"},"exec_count":24,"metadata":{"image/png":{"height":623,"width":997}},"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":32,"id":"86a324","input":"from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select features\nX = df_balanced[['real_period', 'rank', 'torsion']]\n\n# Perform K-Means\nkmeans = KMeans(n_clusters=5, random_state=42)\ndf_balanced['Cluster'] = kmeans.fit_predict(X)\n\n# Visualize clusters\nsns.pairplot(df_balanced, vars=['real_period', 'rank', 'torsion'], hue='Cluster', palette='tab10')\nplt.show()","output":{"0":{"data":{"image/png":"3329b26fdcd49f6ba36b7397af46d18a1e6cb06b","text/plain":"<Figure size 810.361x750 with 12 Axes>"},"exec_count":32,"metadata":{"image/png":{"height":741,"width":808}},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"76c754","input":"bsd_features = ['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product', 'sha']\n\ndf_balanced_bsd = df_balanced[bsd_features].copy()","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"c8f9f4","input":"df.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>conductor</th>\n      <th>special_value</th>\n      <th>torsion</th>\n      <th>regulator</th>\n      <th>real_period</th>\n      <th>tamagawa_product</th>\n      <th>sha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>11</td>\n      <td>0.25384</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.25384</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>11</td>\n      <td>0.25384</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>1.26921</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>11</td>\n      <td>0.25384</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>6.34605</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>14</td>\n      <td>0.33022</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.66045</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>14</td>\n      <td>0.33022</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.66045</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   rank  conductor  special_value  torsion  regulator  real_period  \\\n0     0         11        0.25384        1        1.0      0.25384   \n1     0         11        0.25384        5        1.0      1.26921   \n2     0         11        0.25384        5        1.0      6.34605   \n3     0         14        0.33022        2        1.0      0.66045   \n4     0         14        0.33022        2        1.0      0.66045   \n\n   tamagawa_product  sha  \n0                 1    1  \n1                 5    1  \n2                 1    1  \n3                 2    1  \n4                 2    1  "},"exec_count":6,"output_type":"execute_result"}},"pos":3,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"271b02","input":"df_balanced_bsd.head(5)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>special_value</th>\n      <th>torsion</th>\n      <th>real_period</th>\n      <th>regulator</th>\n      <th>tamagawa_product</th>\n      <th>sha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>990015</th>\n      <td>0.91098</td>\n      <td>2</td>\n      <td>0.22775</td>\n      <td>1.00000</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1738635</th>\n      <td>6.46522</td>\n      <td>1</td>\n      <td>0.35255</td>\n      <td>4.58456</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1254341</th>\n      <td>4.56263</td>\n      <td>2</td>\n      <td>1.14066</td>\n      <td>1.00000</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2932794</th>\n      <td>2.93596</td>\n      <td>1</td>\n      <td>0.09175</td>\n      <td>1.00000</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1682714</th>\n      <td>6.31030</td>\n      <td>2</td>\n      <td>0.39439</td>\n      <td>1.00000</td>\n      <td>16</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         special_value  torsion  real_period  regulator  tamagawa_product  sha\n990015         0.91098        2      0.22775    1.00000                 4    4\n1738635        6.46522        1      0.35255    4.58456                 1    4\n1254341        4.56263        2      1.14066    1.00000                 4    4\n2932794        2.93596        1      0.09175    1.00000                 8    4\n1682714        6.31030        2      0.39439    1.00000                16    4"},"exec_count":7,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"a74828","input":"import numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Splitting features and target\nX = df_balanced_bsd[['special_value', 'torsion', 'real_period', 'regulator', 'tamagawa_product']]\ny = df_balanced_bsd['sha']\n\n# 80/20 train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the LightGBM model\nmodel = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","output":{"0":{"name":"stdout","output_type":"stream","text":"[LightGBM] [Info] Number of positive: 40429, number of negative: 40255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 952\n[LightGBM] [Info] Number of data points in the train set: 80684, number of used features: 5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501078 -> initscore=0.004313\n[LightGBM] [Info] Start training from score 0.004313\n"},"1":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.9831945270672219\n"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"b0757d","input":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute the correlation matrix\ncorr = df[['real_period', 'rank', 'torsion']].corr()\n\n# Create a heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\", square=True)\nplt.title(\"Correlation Heatmap: real_period, rank, and torsion\")\nplt.show()","output":{"0":{"data":{"image/png":"4500fc9775b4efeb9665086f9fb0a5b9b6209ffb","text/plain":"<Figure size 600x400 with 2 Axes>"},"exec_count":8,"metadata":{"image/png":{"height":373,"width":459}},"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"acef2e","input":"from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Train a Histogram-based Gradient Boosting classifier\nmodel = HistGradientBoostingClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(y_test, y_pred))","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy: 0.983739837398374\n              precision    recall  f1-score   support\n\n           4       0.98      0.99      0.98     10173\n           9       0.99      0.98      0.98      9999\n\n    accuracy                           0.98     20172\n   macro avg       0.98      0.98      0.98     20172\nweighted avg       0.98      0.98      0.98     20172\n\n"}},"pos":25,"type":"cell"}
{"id":0,"time":1734299625972,"type":"user"}
{"last_load":1732241523790,"type":"file"}