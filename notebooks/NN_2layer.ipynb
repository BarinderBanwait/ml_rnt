{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from scipy.stats import ranksums\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### utilities here (move to new file eventually)\n",
    "column_sums=pd.read_csv('column_sums.csv')\n",
    "column_sums.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "class LogNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def isDivis(N, p):\n",
    "    if N % p == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def isDivisp(N, p):\n",
    "    if N % p == 0:\n",
    "        return 1/p\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def addRand():\n",
    "    return random.random()\n",
    "    \n",
    "def preprocess(df, addcond=False , addprimeord=False, addrecipp=False, addRandom=0, drawrand=False, column_sums=False):\n",
    "    X=df.drop(columns=['rank'])\n",
    "    y=df['rank']\n",
    "\n",
    "    column_names=df.columns.to_list()[:-2]\n",
    "    column_names=[int(item) for item in column_names]\n",
    "    \n",
    "    if addprimeord:      \n",
    "        new_columns={}\n",
    "        for value in column_names:\n",
    "            column_name = f'isDivis_{value}'\n",
    "            new_columns[column_name] = X['conductor'].apply(lambda N: isDivis(int(N), value))\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    if addRandom>0:    \n",
    "        new_columns={}  \n",
    "        for value in range(addRandom):\n",
    "            column_name = f'randval_{value}'\n",
    "            new_columns[column_name] = X['conductor'].apply(lambda N: addRand())\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns)], axis=1)\n",
    "    \n",
    "    if drawrand:\n",
    "        new_columns={}\n",
    "        for value in column_names:\n",
    "            column_name = f'isDivis_{value}'\n",
    "            prob=column_sums.loc['isDivis_'+str(value)].values[0]\n",
    "            new_columns[column_name] = np.random.choice([0, 1], size=X.shape[0], p=[1-prob, prob])\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns, index=X.index)], axis=1)\n",
    "\n",
    "\n",
    "    if addrecipp:\n",
    "        new_columns={}\n",
    "        for value in column_names:\n",
    "            column_name = f'isDivisp_{value}'\n",
    "            new_columns[column_name] = X['conductor'].apply(lambda N: isDivisp(int(N), value))\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns)], axis=1)\n",
    "    \n",
    "    if not addcond:\n",
    "        X=X.drop(columns=['conductor'])\n",
    "    else:\n",
    "        X['conductor']=np.log(X['conductor'])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def gettens(X, y, random_state=42, test_size=0.2, batch_size=32):\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "    #Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train(model, train_dataloader, input_size, num_classes, num_epochs=40, lr=0.001):\n",
    "    # Instantiate the model, define the loss function and the optimizer\n",
    "    model = model(input_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model_with_mcc(model, test_dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Store labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute MCC\n",
    "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
    "    return mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the big dataset with 1000 a_p's and 80000 curves..\n"
     ]
    }
   ],
   "source": [
    "path = r\"../data_files/ecq_B_1000_all_one_per_iso_262143_524287_sample_80k.parquet\"\n",
    "df = utils.load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[(df['conductor']>= 400000)]\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>23</th>\n",
       "      <th>29</th>\n",
       "      <th>...</th>\n",
       "      <th>7867</th>\n",
       "      <th>7873</th>\n",
       "      <th>7877</th>\n",
       "      <th>7879</th>\n",
       "      <th>7883</th>\n",
       "      <th>7901</th>\n",
       "      <th>7907</th>\n",
       "      <th>7919</th>\n",
       "      <th>conductor</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2100582</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2887</td>\n",
       "      <td>-0.2236</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.4170</td>\n",
       "      <td>-0.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>-0.9129</td>\n",
       "      <td>0.6648</td>\n",
       "      <td>0.4506</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.5513</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>-0.1124</td>\n",
       "      <td>485160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077622</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8944</td>\n",
       "      <td>-0.1890</td>\n",
       "      <td>0.4523</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.9383</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3495</td>\n",
       "      <td>-0.4283</td>\n",
       "      <td>-0.1690</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>-0.7771</td>\n",
       "      <td>-0.7425</td>\n",
       "      <td>-0.7647</td>\n",
       "      <td>-0.1011</td>\n",
       "      <td>479791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033639</th>\n",
       "      <td>0.3536</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4472</td>\n",
       "      <td>-0.3780</td>\n",
       "      <td>-0.6030</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4170</td>\n",
       "      <td>-0.1857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4510</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.2366</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6523</td>\n",
       "      <td>0.4944</td>\n",
       "      <td>469386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848068</th>\n",
       "      <td>-0.7071</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>-0.3015</td>\n",
       "      <td>-0.5547</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>-0.1147</td>\n",
       "      <td>0.8341</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>-0.4846</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>-0.5295</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>-0.5906</td>\n",
       "      <td>-0.3318</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>425277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158368</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2887</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.3015</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4453</td>\n",
       "      <td>-0.1409</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>-0.2929</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.7425</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>-0.2697</td>\n",
       "      <td>498624</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846446</th>\n",
       "      <td>0.3536</td>\n",
       "      <td>-0.2887</td>\n",
       "      <td>0.2236</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>-0.1043</td>\n",
       "      <td>0.4642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7216</td>\n",
       "      <td>0.5297</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>-0.1972</td>\n",
       "      <td>-0.0338</td>\n",
       "      <td>-0.8325</td>\n",
       "      <td>-0.4892</td>\n",
       "      <td>0.3371</td>\n",
       "      <td>424886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126163</th>\n",
       "      <td>-0.3536</td>\n",
       "      <td>-0.5774</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.1890</td>\n",
       "      <td>-0.9045</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2425</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.1043</td>\n",
       "      <td>-0.1857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2931</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.7098</td>\n",
       "      <td>0.4281</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>-0.2025</td>\n",
       "      <td>-0.7872</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>491050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927411</th>\n",
       "      <td>0.7071</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>-0.6708</td>\n",
       "      <td>0.7559</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>0.8321</td>\n",
       "      <td>0.3638</td>\n",
       "      <td>-0.2294</td>\n",
       "      <td>-0.8341</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3495</td>\n",
       "      <td>0.5297</td>\n",
       "      <td>-0.1746</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>-0.1295</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>444103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147223</th>\n",
       "      <td>-0.3536</td>\n",
       "      <td>0.2887</td>\n",
       "      <td>0.4472</td>\n",
       "      <td>-0.3780</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>-0.2425</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4170</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9471</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>-0.4394</td>\n",
       "      <td>0.1352</td>\n",
       "      <td>-0.7096</td>\n",
       "      <td>-0.1688</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>-0.2697</td>\n",
       "      <td>496014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058862</th>\n",
       "      <td>-0.3536</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4472</td>\n",
       "      <td>-0.1890</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>-0.8356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3495</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>-0.1746</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>-0.0675</td>\n",
       "      <td>0.6293</td>\n",
       "      <td>475254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33596 rows Ã— 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              2       3       5       7      11      13      17      19  \\\n",
       "index                                                                     \n",
       "2100582  0.0000 -0.2887 -0.2236  0.0000  0.0000  0.1387  0.4851  0.2294   \n",
       "2077622  0.0000  0.0000  0.8944 -0.1890  0.4523  0.0000  0.1213  0.0000   \n",
       "2033639  0.3536  0.0000 -0.4472 -0.3780 -0.6030  0.8321  0.2425  0.0000   \n",
       "1848068 -0.7071  0.0000  0.0000  0.7559 -0.3015 -0.5547  0.2425 -0.1147   \n",
       "2158368  0.0000 -0.2887  0.0000  0.0000 -0.3015  0.1387  0.7276  0.5735   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1846446  0.3536 -0.2887  0.2236  0.1890  0.1508  0.5547  0.9701  0.5735   \n",
       "2126163 -0.3536 -0.5774  0.0000 -0.1890 -0.9045  0.0000 -0.2425  0.4588   \n",
       "1927411  0.7071  0.5774 -0.6708  0.7559 -0.1508  0.8321  0.3638 -0.2294   \n",
       "2147223 -0.3536  0.2887  0.4472 -0.3780  0.3015  0.5547 -0.2425  0.0000   \n",
       "2058862 -0.3536  0.0000  0.4472 -0.1890  0.1508  0.1387  0.9701  0.6882   \n",
       "\n",
       "             23      29  ...    7867    7873    7877    7879    7883    7901  \\\n",
       "index                    ...                                                   \n",
       "2100582  0.4170 -0.5571  ... -0.1917 -0.9129  0.6648  0.4506  0.1126  0.5513   \n",
       "2077622 -0.9383 -0.3714  ... -0.3495 -0.4283 -0.1690 -0.0225 -0.7771 -0.7425   \n",
       "2033639 -0.4170 -0.1857  ... -0.4510  0.7776  0.2366  0.0225  0.6533  0.0000   \n",
       "1848068  0.8341  0.9285  ...  0.1466 -0.4846  0.9183 -0.5295  0.2196 -0.5906   \n",
       "2158368  0.6255  0.0000  ... -0.4453 -0.1409  0.1014 -0.2929  0.0788  0.7425   \n",
       "...         ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "1846446 -0.1043  0.4642  ...  0.7216  0.5297  0.1859 -0.1972 -0.0338 -0.8325   \n",
       "2126163  0.1043 -0.1857  ... -0.2931  0.5748  0.7098  0.4281  0.4167 -0.2025   \n",
       "1927411 -0.8341 -0.3714  ... -0.3495  0.5297 -0.1746  0.1465 -0.1295 -0.0225   \n",
       "2147223  0.4170  0.5571  ... -0.9471  0.1691 -0.4394  0.1352 -0.7096 -0.1688   \n",
       "2058862  0.3128 -0.8356  ...  0.3495  0.8453 -0.1746  0.1465  0.2309  0.0506   \n",
       "\n",
       "           7907    7919  conductor  rank  \n",
       "index                                     \n",
       "2100582  0.0112 -0.1124     485160     0  \n",
       "2077622 -0.7647 -0.1011     479791     1  \n",
       "2033639 -0.6523  0.4944     469386     1  \n",
       "1848068 -0.3318  0.0225     425277     1  \n",
       "2158368  0.1125 -0.2697     498624     1  \n",
       "...         ...     ...        ...   ...  \n",
       "1846446 -0.4892  0.3371     424886     1  \n",
       "2126163 -0.7872  0.1011     491050     1  \n",
       "1927411  0.7422  0.7023     444103     1  \n",
       "2147223  0.6523 -0.2697     496014     0  \n",
       "2058862 -0.0675  0.6293     475254     1  \n",
       "\n",
       "[33596 rows x 1002 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, run a logistic regression model just with the first 1000 ap coefficients, then one with the first 1000 ap's as well as information on prime divisors, then one where the prime divisors are sampled at random based on the probability distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 1000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for just the first 1000 columns\n",
    "addcond=False\n",
    "addprimeord=False\n",
    "model=LogNN\n",
    "drawrand=False\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "mcc_1000aps_log=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(LogNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_1000aps.append(mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 2000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for the first 1000 ap values + random prime divisor data\n",
    "addcond=False\n",
    "addprimeord=False\n",
    "model=LogNN\n",
    "drawrand=True\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "mcc_with_rand_primes_log=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(LogNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_with_rand_primes.append(mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 2000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for a model with the first 1000 ap values + prime divisors\n",
    "addcond=False\n",
    "addprimeord=True\n",
    "model=LogNN\n",
    "drawrand=False\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "\n",
    "mcc_with_prime_divs_log=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(LogNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_with_prime_divs.append(mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run A/B testing on the pairs (1000 ap's, 1000 ap's + random prime divisors) and (1000 ap's + random prime divisors, 1000 ap's + prime divisor data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean MCC for 100 trials of the 1000 ap values using logistic regression is: 0.7934717858038397 and standard deviation 0.005626594809459237\n",
      "The mean MCC for 100 trials of the 1000 ap values plus random prime divisor data using logistic regression is: 0.7761680498152103 and standard deviation 0.00637751709520485\n",
      "The mean MCC for 100 trials of the 1000 ap values plus correct divisor data using logistic regression is: 0.7845430886681081 and standard deviation 0.0067933702176117845\n"
     ]
    }
   ],
   "source": [
    "print(f\"The mean MCC for 100 trials of the 1000 ap values using logistic regression is: {np.mean(mcc_1000aps_log)} and standard deviation {np.std(mcc_1000aps_log)}\")\n",
    "print(f\"The mean MCC for 100 trials of the 1000 ap values plus random prime divisor data using logistic regression is: {np.mean(mcc_with_rand_primes_log)} and standard deviation {np.std(mcc_with_rand_primes_log)}\")\n",
    "print(f\"The mean MCC for 100 trials of the 1000 ap values plus correct divisor data using logistic regression is: {np.mean(mcc_with_prime_divs_log)} and standard deviation {np.std(mcc_with_prime_divs_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(mcc_1000aps_log, bins=bins, alpha=0.5, label='1000 aps')\n",
    "plt.hist(mcc_with_rand_primes_log, bins=bins, alpha=0.5, label='Random prime divisors')\n",
    "plt.hist(mcc_with_prime_divs_log, bins=bins, alpha=0.5, label='Correct prime divisors')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Histograms of Multiple Data Sets')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test results:\n",
      "mcc_1000aps: ShapiroResult(statistic=0.9804193893647213, pvalue=0.14329885109885293)\n",
      "mcc_with_rand_primes: ShapiroResult(statistic=0.9887944527858573, pvalue=0.5684884399758537)\n",
      "mcc_with_prime_divs: ShapiroResult(statistic=0.9912539498837079, pvalue=0.7649310939691585)\n"
     ]
    }
   ],
   "source": [
    "shapiro_test_1000aps_log = stats.shapiro(mcc_1000aps_log)\n",
    "shapiro_test_rand_primes_log = stats.shapiro(mcc_with_rand_primes_log)\n",
    "shapiro_test_prime_divs_log = stats.shapiro(mcc_with_prime_divs_log)\n",
    "\n",
    "print(\"Shapiro-Wilk test results:\")\n",
    "print(f\"mcc_1000aps_log: {shapiro_test_1000aps_log}\")\n",
    "print(f\"mcc_with_rand_primes_log: {shapiro_test_rand_primes_log}\")\n",
    "print(f\"mcc_with_prime_divs_log: {shapiro_test_prime_divs_log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test results:\n",
      "mcc_1000aps: ShapiroResult(statistic=0.9804193893647213, pvalue=0.14329885109885293)\n",
      "mcc_with_rand_primes: ShapiroResult(statistic=0.9887944527858573, pvalue=0.5684884399758537)\n",
      "mcc_with_prime_divs: ShapiroResult(statistic=0.9912539498837079, pvalue=0.7649310939691585)\n",
      "T-test result (mcc_1000aps vs mcc_with_rand_primes):\n",
      "T-statistic: 20.243908322157253\n",
      "P-value: 4.1118134135781107e-50\n",
      "Reject the null hypothesis: There is a significant difference between the two lists.\n",
      "T-test result (mcc_1000aps vs mcc_with_prime_divs):\n",
      "T-statistic: 10.07145722451518\n",
      "P-value: 1.5848358087218002e-19\n",
      "Reject the null hypothesis: There is a significant difference between the two lists.\n",
      "T-test result (mcc_with_rand_primes vs mcc_with_prime_divs):\n",
      "T-statistic: -8.94309874764402\n",
      "P-value: 2.6978714931505945e-16\n",
      "Reject the null hypothesis: There is a significant difference between the two lists.\n"
     ]
    }
   ],
   "source": [
    "# Perform T-test if data is normally distributed, and a non-parametric test otherwise.\n",
    "alpha = 0.05\n",
    "\n",
    "if shapiro_test_1000aps_log.pvalue > 0.05 and shapiro_test_rand_primes_log.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using just 1000 ap values and adding random prime divisor data.\")\n",
    "    t_stat_log, p_value_log = stats.ttest_ind(mcc_1000aps_log, mcc_with_rand_primes_log)\n",
    "    print(f\"T-test result (mcc_1000aps_lpg vs mcc_with_rand_primes_log):\")\n",
    "    print(f'T-statistic: {t_stat_log}')\n",
    "    print(f'P-value: {p_value_log}')\n",
    "    if p_value_log < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding random prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_1000aps_log, mcc_with_rand_primes_log)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "    \n",
    "\n",
    "if shapiro_test_1000aps_log.pvalue > 0.05 and shapiro_test_prime_divs_log.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using just 1000 ap values and adding correct prime divisor data..\")\n",
    "    t_stat_log, p_value_log = stats.ttest_ind(mcc_1000aps_log, mcc_with_prime_divs_log)\n",
    "    print(f\"T-test result (mcc_1000aps vs mcc_with_prime_divs):\")\n",
    "    print(f'T-statistic: {t_stat_log}')\n",
    "    print(f'P-value: {p_value_log}')\n",
    "    if p_value_log < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding correct prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_1000aps_log, mcc_with_prime_divs_log)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "    \n",
    "\n",
    "\n",
    "if shapiro_test_1000aps_log.pvalue > 0.05 and shapiro_test_prime_divs_log.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using randomand correct prime divisor data.\")\n",
    "    t_stat_log, p_value_log = stats.ttest_ind(mcc_with_rand_primes_log, mcc_with_prime_divs_log)\n",
    "    print(f\"T-test result (mcc_with_rand_primes vs mcc_with_prime_divs):\")\n",
    "    print(f'T-statistic: {t_stat_log}')\n",
    "    print(f'P-value: {p_value_log}')\n",
    "    if p_value_log < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using randomand correct prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_with_rand_primes_log, mcc_with_prime_divs_log)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, do the same, now running a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 1000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for just the first 1000 columns\n",
    "addcond=False\n",
    "addprimeord=False\n",
    "model=LogNN\n",
    "drawrand=False\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "mcc_1000aps_NN=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(SimpleNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_1000aps_NN.append(mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 2000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for the first 1000 ap values + random prime divisor data\n",
    "addcond=False\n",
    "addprimeord=False\n",
    "model=LogNN\n",
    "drawrand=True\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "mcc_with_rand_primes_NN=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(SimpleNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_with_rand_primes_NN.append(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33596, 2000)\n"
     ]
    }
   ],
   "source": [
    "### set up parameters for a model with the first 1000 ap values + prime divisors\n",
    "addcond=False\n",
    "addprimeord=True\n",
    "model=LogNN\n",
    "drawrand=False\n",
    "\n",
    "X, y = preprocess(df1, addcond=addcond, addprimeord=addprimeord, drawrand=drawrand, column_sums=column_sums)\n",
    "input_size = X.shape[1]\n",
    "print(X.shape)\n",
    "\n",
    "#print((X.iloc[:, -1000:].sum(axis=0)/X.shape[0]).head(25))\n",
    "#print(column_sums.head(25))\n",
    "\n",
    "\n",
    "mcc_with_prime_divs_NN=[]\n",
    "for i in range(100):     \n",
    "    num_classes = len(y.unique())\n",
    "    train_dataloader, test_dataloader = gettens(X, y, random_state=i)\n",
    "    model = train(SimpleNN, train_dataloader, input_size, num_classes)\n",
    "    mcc = evaluate_model_with_mcc(model, test_dataloader)\n",
    "    mcc_with_prime_divs_NN.append(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean MCC for 100 trials of the 1000 ap values using logistic regression is: 0.8016059854358448 and standard deviation 0.011264933723214781\n",
      "The mean MCC for 100 trials of the 1000 ap values plus random prime divisor data using logistic regression is: 0.8027217885961724 and standard deviation 0.012765710143521033\n",
      "The mean MCC for 100 trials of the 1000 ap values plus correct divisor data using logistic regression is: 0.8166112703742996 and standard deviation 0.01065341380609778\n"
     ]
    }
   ],
   "source": [
    "print(f\"The mean MCC for 100 trials of the 1000 ap values using logistic regression is: {np.mean(mcc_1000aps_NN)} and standard deviation {np.std(mcc_1000aps_NN)}\")\n",
    "print(f\"The mean MCC for 100 trials of the 1000 ap values plus random prime divisor data using logistic regression is: {np.mean(mcc_with_rand_primes_NN)} and standard deviation {np.std(mcc_with_rand_primes_NN)}\")\n",
    "print(f\"The mean MCC for 100 trials of the 1000 ap values plus correct divisor data using logistic regression is: {np.mean(mcc_with_prime_divs_NN)} and standard deviation {np.std(mcc_with_prime_divs_NN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = 10\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(mcc_1000aps_NN, bins=bins, alpha=0.5, label='1000 aps')\n",
    "plt.hist(mcc_with_rand_primes_NN, bins=bins, alpha=0.5, label='Random prime divisors')\n",
    "plt.hist(mcc_with_prime_divs_NN, bins=bins, alpha=0.5, label='Correct prime divisors')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Histograms of Multiple Data Sets')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test results:\n",
      "mcc_1000aps: ShapiroResult(statistic=0.8838135027660748, pvalue=2.6410047100518967e-07)\n",
      "mcc_with_rand_primes: ShapiroResult(statistic=0.9240774909642739, pvalue=2.3316166489628912e-05)\n",
      "mcc_with_prime_divs: ShapiroResult(statistic=0.9854273020743147, pvalue=0.34078198972033963)\n"
     ]
    }
   ],
   "source": [
    "shapiro_test_1000aps_NN = stats.shapiro(mcc_1000aps_NN)\n",
    "shapiro_test_rand_primes_NN = stats.shapiro(mcc_with_rand_primes_NN)\n",
    "shapiro_test_prime_divs_NN = stats.shapiro(mcc_with_prime_divs_NN)\n",
    "\n",
    "print(\"Shapiro-Wilk test results:\")\n",
    "print(f\"mcc_1000aps: {shapiro_test_1000aps_NN}\")\n",
    "print(f\"mcc_with_rand_primes: {shapiro_test_rand_primes_NN}\")\n",
    "print(f\"mcc_with_prime_divs: {shapiro_test_prime_divs_NN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding random prime divisor data.\n",
      "Fail to reject the null hypothesis: There is no significant difference between the two lists.\n",
      "Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding correct prime divisor data.\n",
      "Reject the null hypothesis: There is a significant difference between the two lists.\n",
      "Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using randomand correct prime divisor data.\n",
      "Reject the null hypothesis: There is a significant difference between the two lists.\n"
     ]
    }
   ],
   "source": [
    "# Perform T-test if data is normally distributed\n",
    "alpha = 0.05\n",
    "\n",
    "if shapiro_test_1000aps_NN.pvalue > 0.05 and shapiro_test_rand_primes_NN.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using just 1000 ap values and adding random prime divisor data.\")\n",
    "    t_stat_NN, p_value_NN = stats.ttest_ind(mcc_1000aps_NN, mcc_with_rand_primes_NN)\n",
    "    print(f\"T-test result (mcc_1000aps_lpg vs mcc_with_rand_primes_NN):\")\n",
    "    print(f'T-statistic: {t_stat_NN}')\n",
    "    print(f'P-value: {p_value_NN}')\n",
    "    if p_value_NN < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding random prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_1000aps_NN, mcc_with_rand_primes_NN)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "    \n",
    "\n",
    "if shapiro_test_1000aps_NN.pvalue > 0.05 and shapiro_test_prime_divs_NN.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using just 1000 ap values and adding correct prime divisor data..\")\n",
    "    t_stat_NN, p_value_NN = stats.ttest_ind(mcc_1000aps_NN, mcc_with_prime_divs_NN)\n",
    "    print(f\"T-test result (mcc_1000aps vs mcc_with_prime_divs):\")\n",
    "    print(f'T-statistic: {t_stat_NN}')\n",
    "    print(f'P-value: {p_value_NN}')\n",
    "    if p_value_NN < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using just 1000 ap values and adding correct prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_1000aps_NN, mcc_with_prime_divs_NN)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "    \n",
    "\n",
    "\n",
    "if shapiro_test_1000aps_NN.pvalue > 0.05 and shapiro_test_prime_divs_NN.pvalue > 0.05:\n",
    "    print(\"Data is normally distributed. Using a t-test to determine if there is a significant difference between using randomand correct prime divisor data.\")\n",
    "    t_stat_NN, p_value_NN = stats.ttest_ind(mcc_with_rand_primes_NN, mcc_with_prime_divs_NN)\n",
    "    print(f\"T-test result (mcc_with_rand_primes vs mcc_with_prime_divs):\")\n",
    "    print(f'T-statistic: {t_stat_NN}')\n",
    "    print(f'P-value: {p_value_NN}')\n",
    "    if p_value_NN < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Using a non-parametric test (the Wilcoxon rank-sum test) to determine if there is a significant difference between using randomand correct prime divisor data.\")\n",
    "    rank_stat, rank_p_value = ranksums(mcc_with_rand_primes_NN, mcc_with_prime_divs_NN)\n",
    "    if rank_p_value < alpha:\n",
    "        print(\"Reject the null hypothesis: There is a significant difference between the two lists.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: There is no significant difference between the two lists.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
