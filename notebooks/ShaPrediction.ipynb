{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import models\n",
    "import executor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from utils import perfect_square_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset with 118 features and 3064705 curves..\n"
     ]
    }
   ],
   "source": [
    "# load your data here. The following ensure this will work on Windows as well as Unix\n",
    "# the data file has its Kodaira symbols already preprocessed\n",
    "path = Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_kodaira_processed.parquet\"\n",
    "df = utils.load_data(path)\n",
    "\n",
    "# process kodaira symbol if it is not already done\n",
    "# path = Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000.parquet\"\n",
    "# df = utils.load_data(path)\n",
    "# df = utils.process_kodaira_symbol(df)\n",
    "# df.drop('lmfdb_label', axis=1, inplace=True)\n",
    "# df.to_parquet(Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_kodaira_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>23</th>\n",
       "      <th>29</th>\n",
       "      <th>...</th>\n",
       "      <th>tamagawa_product</th>\n",
       "      <th>kodaira_-5.0</th>\n",
       "      <th>kodaira_-4.0</th>\n",
       "      <th>kodaira_-3.0</th>\n",
       "      <th>kodaira_-2.0</th>\n",
       "      <th>kodaira_-1.0</th>\n",
       "      <th>kodaira_2.0</th>\n",
       "      <th>kodaira_3.0</th>\n",
       "      <th>kodaira_4.0</th>\n",
       "      <th>kodaira_5.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.70711</td>\n",
       "      <td>-0.28868</td>\n",
       "      <td>0.22361</td>\n",
       "      <td>-0.37796</td>\n",
       "      <td>0.15076</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>-0.24254</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.10426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.70711</td>\n",
       "      <td>-0.28868</td>\n",
       "      <td>0.22361</td>\n",
       "      <td>-0.37796</td>\n",
       "      <td>0.15076</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>-0.24254</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.10426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.70711</td>\n",
       "      <td>-0.28868</td>\n",
       "      <td>0.22361</td>\n",
       "      <td>-0.37796</td>\n",
       "      <td>0.15076</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>-0.24254</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.10426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.18898</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.5547</td>\n",
       "      <td>0.72761</td>\n",
       "      <td>0.22942</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.55709</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.35355</td>\n",
       "      <td>-0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.18898</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.5547</td>\n",
       "      <td>0.72761</td>\n",
       "      <td>0.22942</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.55709</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2        3        5        7       11      13       17       19  \\\n",
       "0 -0.70711 -0.28868  0.22361 -0.37796  0.15076  0.5547 -0.24254  0.00000   \n",
       "1 -0.70711 -0.28868  0.22361 -0.37796  0.15076  0.5547 -0.24254  0.00000   \n",
       "2 -0.70711 -0.28868  0.22361 -0.37796  0.15076  0.5547 -0.24254  0.00000   \n",
       "3 -0.35355 -0.57735  0.00000  0.18898  0.00000 -0.5547  0.72761  0.22942   \n",
       "4 -0.35355 -0.57735  0.00000  0.18898  0.00000 -0.5547  0.72761  0.22942   \n",
       "\n",
       "        23       29  ...  tamagawa_product  kodaira_-5.0  kodaira_-4.0  \\\n",
       "0 -0.10426  0.00000  ...                 1             0             0   \n",
       "1 -0.10426  0.00000  ...                 5             0             0   \n",
       "2 -0.10426  0.00000  ...                 1             0             0   \n",
       "3  0.00000 -0.55709  ...                 2             0             0   \n",
       "4  0.00000 -0.55709  ...                 2             0             0   \n",
       "\n",
       "   kodaira_-3.0  kodaira_-2.0  kodaira_-1.0  kodaira_2.0  kodaira_3.0  \\\n",
       "0             0             0             0            0            0   \n",
       "1             0             0             0            0            0   \n",
       "2             0             0             0            0            0   \n",
       "3             0             0             0            0            0   \n",
       "4             0             0             0            0            0   \n",
       "\n",
       "   kodaira_4.0  kodaira_5.0  \n",
       "0            0            1  \n",
       "1            0            1  \n",
       "2            0            1  \n",
       "3            0            1  \n",
       "4            0            1  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2', '3', '5', '7', '11', '13', '17', '19', '23', '29',\n",
       "       ...\n",
       "       'tamagawa_product', 'kodaira_-5.0', 'kodaira_-4.0', 'kodaira_-3.0',\n",
       "       'kodaira_-2.0', 'kodaira_-1.0', 'kodaira_2.0', 'kodaira_3.0',\n",
       "       'kodaira_4.0', 'kodaira_5.0'],\n",
       "      dtype='object', length=119)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '3',\n",
       " '5',\n",
       " '7',\n",
       " '11',\n",
       " '13',\n",
       " '17',\n",
       " '19',\n",
       " '23',\n",
       " '29',\n",
       " '31',\n",
       " '37',\n",
       " '41',\n",
       " '43',\n",
       " '47',\n",
       " '53',\n",
       " '59',\n",
       " '61',\n",
       " '67',\n",
       " '71',\n",
       " '73',\n",
       " '79',\n",
       " '83',\n",
       " '89',\n",
       " '97',\n",
       " '101',\n",
       " '103',\n",
       " '107',\n",
       " '109',\n",
       " '113',\n",
       " '127',\n",
       " '131',\n",
       " '137',\n",
       " '139',\n",
       " '149',\n",
       " '151',\n",
       " '157',\n",
       " '163',\n",
       " '167',\n",
       " '173',\n",
       " '179',\n",
       " '181',\n",
       " '191',\n",
       " '193',\n",
       " '197',\n",
       " '199',\n",
       " '211',\n",
       " '223',\n",
       " '227',\n",
       " '229',\n",
       " '233',\n",
       " '239',\n",
       " '241',\n",
       " '251',\n",
       " '257',\n",
       " '263',\n",
       " '269',\n",
       " '271',\n",
       " '277',\n",
       " '281',\n",
       " '283',\n",
       " '293',\n",
       " '307',\n",
       " '311',\n",
       " '313',\n",
       " '317',\n",
       " '331',\n",
       " '337',\n",
       " '347',\n",
       " '349',\n",
       " '353',\n",
       " '359',\n",
       " '367',\n",
       " '373',\n",
       " '379',\n",
       " '383',\n",
       " '389',\n",
       " '397',\n",
       " '401',\n",
       " '409',\n",
       " '419',\n",
       " '421',\n",
       " '431',\n",
       " '433',\n",
       " '439',\n",
       " '443',\n",
       " '449',\n",
       " '457',\n",
       " '461',\n",
       " '463',\n",
       " '467',\n",
       " '479',\n",
       " '487',\n",
       " '491',\n",
       " '499',\n",
       " '503',\n",
       " '509',\n",
       " '521',\n",
       " '523',\n",
       " '541',\n",
       " 'conductor',\n",
       " 'rank',\n",
       " 'torsion',\n",
       " 'adelic_level',\n",
       " 'adelic_index',\n",
       " 'adelic_genus',\n",
       " 'sha',\n",
       " 'real_period',\n",
       " 'special_value',\n",
       " 'tamagawa_product',\n",
       " 'kodaira_-5.0',\n",
       " 'kodaira_-4.0',\n",
       " 'kodaira_-3.0',\n",
       " 'kodaira_-2.0',\n",
       " 'kodaira_-1.0',\n",
       " 'kodaira_2.0',\n",
       " 'kodaira_3.0',\n",
       " 'kodaira_4.0',\n",
       " 'kodaira_5.0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "[x for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['conductor'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       92.053819\n",
       "4        5.180205\n",
       "9        1.645444\n",
       "16       0.592879\n",
       "25       0.277384\n",
       "36       0.078376\n",
       "49       0.070251\n",
       "64       0.042451\n",
       "81       0.019415\n",
       "100      0.010278\n",
       "121      0.009789\n",
       "169      0.005221\n",
       "144      0.004731\n",
       "196      0.001893\n",
       "256      0.001468\n",
       "225      0.001403\n",
       "289      0.001109\n",
       "361      0.000946\n",
       "400      0.000489\n",
       "529      0.000424\n",
       "625      0.000392\n",
       "324      0.000228\n",
       "784      0.000228\n",
       "441      0.000163\n",
       "1024     0.000131\n",
       "729      0.000131\n",
       "576      0.000131\n",
       "676      0.000098\n",
       "841      0.000098\n",
       "961      0.000098\n",
       "484      0.000065\n",
       "1681     0.000033\n",
       "5625     0.000033\n",
       "2500     0.000033\n",
       "2209     0.000033\n",
       "1089     0.000033\n",
       "1849     0.000033\n",
       "1369     0.000033\n",
       "1156     0.000033\n",
       "Name: sha, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the value counts\n",
    "value_counts = df['sha'].value_counts()\n",
    "\n",
    "# Calculate the percentage of the total for each value count\n",
    "percentage_counts = (value_counts / value_counts.sum()) * 100\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    50428\n",
       "9    50428\n",
       "Name: sha, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a balanced dataset with equal amount of 4 and 9 labels\n",
    "len_9 = df[df['sha'] == 9].shape[0]\n",
    "# df_balanced = df[df['sha'] == 1].sample(len_4)\n",
    "df_balanced = df[df['sha'] == 4].iloc[:len_9]\n",
    "df_balanced = pd.concat([df_balanced, df[df['sha'] == 9]])\n",
    "df_balanced.sha.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now just get a smaller dataset so training is quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    50.0\n",
       "9    50.0\n",
       "Name: sha, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the value counts\n",
    "value_counts = df_balanced['sha'].value_counts()\n",
    "\n",
    "# Calculate the percentage of the total for each value count\n",
    "percentage_counts = (value_counts / value_counts.sum()) * 100\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input dimension is 118 and the output dimension is 2.\n",
      "Device: cpu.\n",
      "The model has 26,082 trainable parameters..\n",
      "VanillaNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=118, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# choose model parameters\n",
    "hidden_units = [128,64,32]\n",
    "\n",
    "# default model parameters\n",
    "input_dim, output_dim = utils.get_input_output_dim(df_balanced, 'sha', if_regression=False)\n",
    "\n",
    "# check if we have cuda available\n",
    "device = utils.get_device()\n",
    "\n",
    "# create model\n",
    "model = models.VanillaNN(input_dim, hidden_units, output_dim, if_dropout=False, dropout_rate=0.5, if_batchnorm=True).to(device)\n",
    "# model = models.VanillaNN(input_dim, hidden_units, output_dim).to(device)\n",
    "\n",
    "# print model summary\n",
    "utils.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose training parameters\n",
    "loss_func = nn.BCELoss()\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "from sklearn.metrics import accuracy_score\n",
    "evaluator = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m train_dataloader, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprepare_data(df_balanced, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msha\u001b[39m\u001b[38;5;124m'\u001b[39m, device, if_regression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model, train_eval_hist, val_eval_hist \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_regression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plot train_eval_hist, val_eval_hist\u001b[39;00m\n\u001b[1;32m      8\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_train_eval_hist(train_eval_hist, val_eval_hist)\n",
      "File \u001b[0;32m~/ml_rnt/notebooks/executor.py:47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m if_regression \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3148\u001b[0m     )\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Map the labels to 0 and 1\n",
    "df_balanced['sha'] = df_balanced['sha'].map({4: 0, 9: 1})\n",
    "# split data\n",
    "train_dataloader, val_dataset, test_dataset = utils.prepare_data(df_balanced, 'sha', device, if_regression=False)\n",
    "# train the model\n",
    "model, train_eval_hist, val_eval_hist = executor.train(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression=False, verbose=True)\n",
    "# plot train_eval_hist, val_eval_hist\n",
    "utils.plot_train_eval_hist(train_eval_hist, val_eval_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a classifier instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_prepare_data(df, target_column, device, if_regression=False, batch_size=32):\n",
    "    # Map target labels to range [0, C-1]\n",
    "    df[target_column] = df[target_column].map({4: 0, 9: 1})\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = df.drop(columns=[target_column]).values\n",
    "    y = df[target_column].values\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target -9223372036854775808 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model, train_eval_hist, val_eval_hist \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_regression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# plot train_eval_hist, val_eval_hist\u001b[39;00m\n\u001b[1;32m     16\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_train_eval_hist(train_eval_hist, val_eval_hist)\n",
      "File \u001b[0;32m~/ml_rnt/notebooks/executor.py:47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m if_regression \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -9223372036854775808 is out of bounds."
     ]
    }
   ],
>>>>>>> f9051e3da0d4b4cb2f26c0c2e5bf7f19d3c1fd16
   "source": [
    "# split data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataloader, val_dataset, test_dataset = my_prepare_data(df_balanced_small, 'sha', device, if_regression=False)\n",
    "\n",
    "# Define the model, loss function, optimizer, and other components\n",
    "input_size = df_balanced_small.drop(columns=['sha']).shape[1]\n",
    "model = models.VanillaNN(input_dim, hidden_units, 2, if_dropout=False, dropout_rate=0.5, if_batchnorm=True).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  # For classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "# train the model\n",
    "model, train_eval_hist, val_eval_hist = executor.train(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression=False, verbose=True)\n",
    "# plot train_eval_hist, val_eval_hist\n",
    "utils.plot_train_eval_hist(train_eval_hist, val_eval_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "executor.test(model, test_dataset, evaluator, if_regression = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just load a trained model and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
>>>>>>> f9051e3da0d4b4cb2f26c0c2e5bf7f19d3c1fd16
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset with 118 features and 3064705 curves..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1147, grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path = Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_kodaira_processed.parquet\"\n",
    "df = utils.load_data(path)\n",
    "\n",
    "# load torch model\n",
    "import torch\n",
    "model = torch.load(Path(\"..\") / \"trained_models\" / \"model.pth\")\n",
    "\n",
    "# split data\n",
    "train_dataloader, val_dataset, test_dataset = utils.prepare_data(df_balanced, 'sha', device, if_regression=True)\n",
    "X_test, y_test = test_dataset.tensors\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "y_pred = outputs.squeeze()\n",
    "\n",
    "# get the l1 distance between the predicted and actual values\n",
    "loss = torch.nn.L1Loss()\n",
    "loss(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
