{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import utils\n",
    "from lib import models\n",
    "from lib import executor\n",
    "from lib.utils import nearest_integer_acc\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process kodaira symbol if it is not already done\n",
    "# path = '../data_files/sha/ecq_sha_B_100_conds_1_500000.parquet'\n",
    "# df = utils.load_data(path)\n",
    "# df = utils.process_kodaira_symbol(df)\n",
    "# df.drop('lmfdb_label', axis=1, inplace=True)\n",
    "# df.to_parquet(Path(\"..\") / \"data_files\" / \"sha\"/ \"ecq_sha_B_100_conds_1_500000_kodaira_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset with 120 features and 3064705 curves..\n",
      "Now, leaving out the rank 0 curves.\n",
      "Now, there are in total 20605 ( 1.09%) curves with sqrt_sha > 1. Values counts of the square root of order of sha: \n",
      "1    1873224\n",
      "2      18710\n",
      "3       1462\n",
      "4        323\n",
      "5         96\n",
      "7          9\n",
      "8          3\n",
      "6          2\n",
      "Name: sqrt_sha, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load your data here. The following ensure this will work on Windows as well as Unix\n",
    "# the data file has its Kodaira symbols already preprocessed \n",
    "path = '../data_files/sha/ecq_sha_B_100_conds_1_500000_reg.parquet'\n",
    "df = utils.load_data(path)\n",
    "\n",
    "# dropping columns that are not needed\n",
    "df.drop(['conductor','adelic_level','lmfdb_label'], axis=1, inplace=True)\n",
    "\n",
    "# get square root of order of sha\n",
    "df['sqrt_sha'] = df['sha'].apply(lambda x: int(x**0.5))\n",
    "df.drop('sha', axis=1, inplace=True)\n",
    "\n",
    "# get rank great than 0 curves\n",
    "df = df[df['rank'] > 0]\n",
    "print('Now, leaving out the rank 0 curves.')\n",
    "\n",
    "# get how many curves are with sqrt_sha > 1\n",
    "n_sha_not_1 = len(df[df['sqrt_sha'] > 1])\n",
    "print(f'Now, there are in total {n_sha_not_1} ({n_sha_not_1/len(df)*100 : .2f}%) curves with sqrt_sha > 1. Values counts of the square root of order of sha: ')\n",
    "print(df.sqrt_sha.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18710\n",
       "2    18710\n",
       "Name: sqrt_sha, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a balanced dataset with equal amount of 1 and 4 labels\n",
    "len_2 = df[df['sqrt_sha'] == 2].shape[0]\n",
    "df_balanced = df[df['sqrt_sha'] == 1].sample(len_2) \n",
    "df_balanced = pd.concat([df_balanced, df[df['sqrt_sha'] == 2]])\n",
    "df_balanced.sqrt_sha.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your features \n",
    "\n",
    "# model BSD\n",
    "# df_balanced=df_balanced[['rank', 'torsion', 'regulator', 'real_period','special_value', 'tamagawa_product','sqrt_sha']]\n",
    "\n",
    "# model BSD (no regulator) + \n",
    "df_balanced.drop('regulator',inplace=True,axis=1)\n",
    "\n",
    "# model BSD (no regulator)\n",
    "# df_balanced=df_balanced[['rank', 'torsion','real_period','special_value', 'tamagawa_product','sqrt_sha']]\n",
    "\n",
    "# model BSD (no regulator + rank)\n",
    "# ['torsion', 'real_period', 'special_value', 'tamagawa_product','sqrt_sha']\n",
    "# df_balanced=df_balanced[['torsion','real_period','special_value', 'tamagawa_product','sqrt_sha']]\n",
    "\n",
    "# model BSD (no rank + real_period)\n",
    "# ['torsion', 'real_period', 'special_value', 'tamagawa_product','sqrt_sha']\n",
    "# df_balanced=df_balanced[['torsion','special_value', 'tamagawa_product','sqrt_sha']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input dimension is 116 and the output dimension is 1.\n",
      "Device: cpu.\n",
      "The model has 25,793 trainable parameters..\n",
      "VanillaNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=116, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# choose model parameters\n",
    "hidden_units = [128,64,32]\n",
    "\n",
    "# default model parameters\n",
    "input_dim, output_dim = utils.get_input_output_dim(df_balanced, 'sqrt_sha', if_regression=True)\n",
    "\n",
    "# check if we have cuda available\n",
    "device = utils.get_device()\n",
    "\n",
    "# create model\n",
    "model = models.VanillaNN(input_dim, hidden_units, output_dim, if_dropout=False, dropout_rate=0.5, if_batchnorm=True).to(device)\n",
    "\n",
    "# print model summary\n",
    "utils.model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose training parameters\n",
    "loss_func = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "evaluator = nearest_integer_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50. Training MSELoss : 0.0142, Validation MSELoss : 0.0145. Training nearest_integer_acc: 0.9888, Validation nearest_integer_acc: 0.9890\n",
      "Epoch 2/50. Training MSELoss : 0.0149, Validation MSELoss : 0.0153. Training nearest_integer_acc: 0.9889, Validation nearest_integer_acc: 0.9891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_dataloader, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprepare_data(df_balanced, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt_sha\u001b[39m\u001b[38;5;124m'\u001b[39m, device, if_regression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model, train_eval_hist, val_eval_hist, train_loss_hist, val_loss_hist \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_regression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# plot train_eval_hist, val_eval_hist\u001b[39;00m\n\u001b[0;32m      6\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_train_eval_hist(train_eval_hist, val_eval_hist)\n",
      "File \u001b[1;32mc:\\Users\\Xiaoyu Huang\\Documents\\ml_rnt\\demo\\lib\\executor.py:62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression, verbose)\u001b[0m\n\u001b[0;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m if_regression \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\Xiaoyu Huang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Xiaoyu Huang\\Documents\\ml_rnt\\demo\\lib\\models.py:55\u001b[0m, in \u001b[0;36mVanillaNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m---> 55\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split data\n",
    "train_dataloader, val_dataset, test_dataset = utils.prepare_data(df_balanced, 'sqrt_sha', device, if_regression=True)\n",
    "# train the model\n",
    "model, train_eval_hist, val_eval_hist, train_loss_hist, val_loss_hist = executor.train(model, train_dataloader, val_dataset, loss_func, evaluator, optimizer, num_epochs, if_regression=True, verbose=True)\n",
    "# plot train_eval_hist, val_eval_hist\n",
    "utils.plot_train_eval_hist(train_eval_hist, val_eval_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_eval_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_train_eval_hist(\u001b[43mtrain_eval_hist\u001b[49m, val_eval_hist)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_eval_hist' is not defined"
     ]
    }
   ],
   "source": [
    "utils.plot_train_eval_hist(train_eval_hist, val_eval_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m utils\u001b[38;5;241m.\u001b[39mplot_train_loss_hist(\u001b[43mtrain_loss_hist\u001b[49m, val_loss_hist)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss_hist' is not defined"
     ]
    }
   ],
   "source": [
    "utils.plot_train_loss_hist(train_loss_hist, val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model(X_test) bigger than 1.5: 536\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test_dataset.tensors\n",
    "# count how many of model(X_test) is bigger than 1.5\n",
    "print(f'Number of model(X_test) bigger than 1.5: {len(model(X_test)[model(X_test) > 1.5])}')\n",
    "# model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9838\n",
      "Test Mean Absolute Error: 0.0555. Test Mean Squared Error: 0.0141\n"
     ]
    }
   ],
   "source": [
    "acc = executor.test(model, test_dataset, evaluator, if_regression = True)\n",
    "mae = executor.test(model, test_dataset, mean_absolute_error, if_regression = True)\n",
    "mse = executor.test(model, test_dataset, mean_squared_error, if_regression = True)\n",
    "print(f\"Test accuracy: {acc:0.4f}\")\n",
    "print(f\"Test Mean Absolute Error: {mae:0.4f}. Test Mean Squared Error: {mse:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model BSD: \n",
    "Test accuracy: 0.9838\n",
    "Test Mean Absolute Error: 0.0555. Test Mean Squared Error: 0.0141\n",
    "\n",
    "model BSD (no regulator) +: \n",
    "Test accuracy: 0.8373\n",
    "Test Mean Absolute Error: 0.2570. Test Mean Squared Error: 0.1181\n",
    "\n",
    "model BSD (no regulator): \n",
    "Test accuracy: 0.8370\n",
    "Test Mean Absolute Error: 0.2680. Test Mean Squared Error: 0.1201\n",
    "\n",
    "model BSD (no regulator + rank):\n",
    "Test accuracy: 0.8280\n",
    "Test Mean Absolute Error: 0.2713. Test Mean Squared Error: 0.1270\n",
    "\n",
    "model BSD (no  regulator + rank + real_period):\n",
    "Test accuracy: 0.7586\n",
    "Test Mean Absolute Error: 0.3639. Test Mean Squared Error: 0.1715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
